{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature based classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we aim to classify the recorded activities using traditional methods, including preprocessing, feature extraction, and classification.\n",
    "\n",
    "Loading relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1754, 2048, 80)\n",
      "Y shape: (1754,)\n",
      "Subjects shape: (1754,)\n"
     ]
    }
   ],
   "source": [
    "x = np.load('scaled_spec_resampled_array.npy')\n",
    "y = np.load('labels_array.npy')-1\n",
    "subjects = np.load('subjects_array.npy')\n",
    "\n",
    "print(f\"X shape: {x.shape}\")\n",
    "print(f\"Y shape: {y.shape}\")\n",
    "print(f\"Subjects shape: {subjects.shape}\")\n",
    "\n",
    "num_samples = x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "nfft = 2048\n",
    "fs = 128000\n",
    "f_lo = -64000.0\n",
    "f_hi = 63937.5\n",
    "\n",
    "freq_axis = np.linspace(f_lo, f_hi, nfft)\n",
    "print(len(freq_axis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising and feature extraction based on this paper:\n",
    "\n",
    "Y. Kim and H. Ling, \"Human Activity Classification Based on Micro-Doppler Signatures Using a Support Vector Machine,\" in IEEE Transactions on Geoscience and Remote Sensing, vol. 47, no. 5, pp. 1328-1337, May 2009\n",
    "\n",
    "Features extracted:\n",
    "\n",
    "(1) Torso Doppler frequency: Indicates the speed of the human subject.\n",
    "\n",
    "(2) Total bandwidth (BW) of the Doppler signal: Relates to the speed of limb motions.\n",
    "\n",
    "(3) Offset of the total Doppler signal: Measures asymmetry between forward and backward limb motions.\n",
    "\n",
    "(4) BW without micro-Dopplers: Represents the Doppler bandwidth of the torso alone.\n",
    "\n",
    "(5) Normalized standard deviation (STD) of the Doppler signal strength: Related to the dynamic range of the motion.\n",
    "\n",
    "(6) Period of limb motion: Corresponds to the swing rate of arms and legs.\n",
    "\n",
    "\n",
    "Also added 2 other features:\n",
    "\n",
    "(7) High envelope standard deviation: The std of the peak frequency can indicate different types of motion. For example, running might show more variability compared to walking.\n",
    "\n",
    "(8) Entropy: How dispersed is the energy across frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_and_extract_features(frequencies, spectrogram, noise_threshold):\n",
    "    # Initialize feature variables\n",
    "    torso_doppler_frequency = []\n",
    "    total_bw_doppler = []\n",
    "    offset_total_doppler = []\n",
    "    bw_without_micro_dopplers = []\n",
    "    normalized_std_doppler = []\n",
    "    entropies = []\n",
    "    high_envelope = []\n",
    "    low_envelope = []\n",
    "\n",
    "    spec_denoised = np.where(spectrogram < noise_threshold, noise_threshold, spectrogram)\n",
    "    for t_index in range(spec_denoised.shape[1]):\n",
    "\n",
    "        column = spec_denoised[:, t_index]\n",
    "        \n",
    "        if np.all(column == noise_threshold):   # If the entire column is noise\n",
    "            high_freq = 0\n",
    "            low_freq = 0\n",
    "            peak_freq = 0\n",
    "            total_bw = 0\n",
    "            offset_total = 0\n",
    "            normalized_std = 0\n",
    "            entr = 0\n",
    "        else:   # If the column contains signal\n",
    "            high_freq = frequencies[column > noise_threshold][-1]\n",
    "            low_freq = frequencies[column > noise_threshold][0]\n",
    "            # Torso Doppler Frequency (1)\n",
    "            peak_freq = frequencies[np.argmax(column)]\n",
    "            # Total Bandwidth of the Doppler Signal (2)\n",
    "            total_bw = high_freq-low_freq\n",
    "            # Offset of the Total Doppler (3)\n",
    "            offset_total = (high_freq + low_freq) / 2\n",
    "            # Normalized STD of the Doppler Signal Strength (5)\n",
    "            std_doppler = np.std(column)\n",
    "            mean_doppler = np.mean(column)\n",
    "            normalized_std = std_doppler / mean_doppler if mean_doppler != 0 else 0\n",
    "            # New feature: Entropy (8)\n",
    "            entr = entropy(column)\n",
    "\n",
    "        high_envelope.append(high_freq)\n",
    "        low_envelope.append(low_freq)\n",
    "        torso_doppler_frequency.append(peak_freq)\n",
    "        total_bw_doppler.append(total_bw)\n",
    "        offset_total_doppler.append(offset_total)\n",
    "        normalized_std_doppler.append(normalized_std)\n",
    "        entropies.append(entr)\n",
    "\n",
    "    # Bandwidth Without Micro-Dopplers (4)\n",
    "    bw_without_micro_dopplers = np.mean(np.array(sorted(high_envelope)[-5:]) - np.array(sorted(low_envelope)[:5]))\n",
    "\n",
    "    # Period of Limb Motion (6)\n",
    "    peaks, _ = find_peaks(high_envelope, height=np.nanmean(high_envelope))\n",
    "    peak_intervals = np.diff(peaks)\n",
    "    periods = peak_intervals / fs\n",
    "    mean_period = np.mean(periods) if len(periods) > 0 else 0\n",
    "\n",
    "    # New feature: Peak frequency standard deviation (7)\n",
    "    peak_freq_variation = np.std(high_envelope)\n",
    "\n",
    "    return [np.mean(torso_doppler_frequency),\n",
    "            np.mean(total_bw_doppler),\n",
    "            np.mean(offset_total_doppler),\n",
    "            bw_without_micro_dopplers,\n",
    "            np.mean(normalized_std_doppler),\n",
    "            mean_period,\n",
    "            np.mean(entropies),\n",
    "            peak_freq_variation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract these features from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1754, 8)\n"
     ]
    }
   ],
   "source": [
    "noise_threshold = -83  # dBm (based on the paper's noise threshold)\n",
    "features = []\n",
    "for i in range(num_samples):\n",
    "    features.append(denoise_and_extract_features(freq_axis, x[i,:,:], noise_threshold))\n",
    "features = np.array(features)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted the features, it is time to split the dataset. Due to the limited dataset size, we chose to perform 5-fold cross-validation to obtain a more accurate estimate of each model's performance.\n",
    "\n",
    "We also note that each of these 5 folds contains activities performed by different subjects, with no overlap across folds. In other words, in each iteration, the test set contains activities from subjects not used in training. This approach simulates a realistic scenario, as in the real world, the subjects in the test set are not seen by the model during training.\n",
    "\n",
    "It's also important to note that the number of activities performed varies across subjects, i.e. some subjects have performed more repetitions than others (see code block below). We account for this variation when splitting the data, resulting in 5 roughly equal folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 8 performed 36 activities\n",
      "Subject 14 performed 36 activities\n",
      "Subject 57 performed 36 activities\n",
      "Subject 30 performed 34 activities\n",
      "Subject 53 performed 34 activities\n",
      "Subject 28 performed 33 activities\n",
      "Subject 29 performed 33 activities\n",
      "Subject 31 performed 33 activities\n",
      "Subject 32 performed 33 activities\n",
      "Subject 33 performed 33 activities\n",
      "Subject 34 performed 33 activities\n",
      "Subject 36 performed 33 activities\n",
      "Subject 37 performed 33 activities\n",
      "Subject 38 performed 33 activities\n",
      "Subject 39 performed 33 activities\n",
      "Subject 40 performed 33 activities\n",
      "Subject 41 performed 33 activities\n",
      "Subject 43 performed 33 activities\n",
      "Subject 44 performed 33 activities\n",
      "Subject 45 performed 33 activities\n",
      "Subject 46 performed 33 activities\n",
      "Subject 47 performed 33 activities\n",
      "Subject 50 performed 33 activities\n",
      "Subject 51 performed 33 activities\n",
      "Subject 52 performed 33 activities\n",
      "Subject 55 performed 33 activities\n",
      "Subject 56 performed 33 activities\n",
      "Subject 35 performed 32 activities\n",
      "Subject 54 performed 32 activities\n",
      "Subject 3 performed 31 activities\n",
      "Subject 11 performed 30 activities\n",
      "Subject 12 performed 30 activities\n",
      "Subject 10 performed 29 activities\n",
      "Subject 42 performed 22 activities\n",
      "Subject 64 performed 19 activities\n",
      "Subject 1 performed 18 activities\n",
      "Subject 2 performed 18 activities\n",
      "Subject 4 performed 18 activities\n",
      "Subject 5 performed 18 activities\n",
      "Subject 6 performed 18 activities\n",
      "Subject 7 performed 18 activities\n",
      "Subject 9 performed 18 activities\n",
      "Subject 13 performed 18 activities\n",
      "Subject 15 performed 18 activities\n",
      "Subject 16 performed 18 activities\n",
      "Subject 17 performed 18 activities\n",
      "Subject 58 performed 18 activities\n",
      "Subject 59 performed 18 activities\n",
      "Subject 60 performed 18 activities\n",
      "Subject 61 performed 18 activities\n",
      "Subject 62 performed 18 activities\n",
      "Subject 65 performed 18 activities\n",
      "Subject 66 performed 18 activities\n",
      "Subject 67 performed 18 activities\n",
      "Subject 68 performed 18 activities\n",
      "Subject 69 performed 18 activities\n",
      "Subject 70 performed 18 activities\n",
      "Subject 71 performed 18 activities\n",
      "Subject 72 performed 18 activities\n",
      "Subject 63 performed 17 activities\n",
      "Subject 18 performed 15 activities\n",
      "Subject 19 performed 15 activities\n",
      "Subject 20 performed 15 activities\n",
      "Subject 22 performed 15 activities\n",
      "Subject 24 performed 15 activities\n",
      "Subject 25 performed 15 activities\n",
      "Subject 26 performed 15 activities\n",
      "Subject 27 performed 15 activities\n",
      "Subject 48 performed 15 activities\n",
      "Subject 49 performed 15 activities\n",
      "Subject 21 performed 14 activities\n",
      "Subject 23 performed 14 activities\n"
     ]
    }
   ],
   "source": [
    "# Number of activities different per subject\n",
    "unique, counts = np.unique(subjects, return_counts=True)\n",
    "subject_counts = dict(zip(unique, counts))\n",
    "sorted_subject_counts = dict(sorted(subject_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "for key, value in sorted_subject_counts.items():\n",
    "    print(f\"Subject {key} performed {value} activities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data in roughly 5 equal folds, ensuring that each subject is only in one fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Length of training set: 1398 samples\n",
      "Length of testing set (fold): 356 samples\n",
      "Unique subjects in test set: [ 4 16 20 23 25 28 32 38 42 46 54 57 58 60 67]\n",
      "Fold 2:\n",
      "Length of training set: 1401 samples\n",
      "Length of testing set (fold): 353 samples\n",
      "Unique subjects in test set: [ 5  8  9 19 21 31 35 39 45 49 52 61 64 68 72]\n",
      "Fold 3:\n",
      "Length of training set: 1405 samples\n",
      "Length of testing set (fold): 349 samples\n",
      "Unique subjects in test set: [ 2  3  7 11 14 18 26 29 37 44 55 65 70 71]\n",
      "Fold 4:\n",
      "Length of training set: 1406 samples\n",
      "Length of testing set (fold): 348 samples\n",
      "Unique subjects in test set: [ 6 12 17 22 34 36 43 47 48 53 56 62 63 69]\n",
      "Fold 5:\n",
      "Length of training set: 1406 samples\n",
      "Length of testing set (fold): 348 samples\n",
      "Unique subjects in test set: [ 1 10 13 15 24 27 30 33 40 41 50 51 59 66]\n"
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=5)\n",
    "splits = list(gkf.split(x, y, groups=subjects))\n",
    "\n",
    "# Verification:\n",
    "for i, (train_index, test_index) in enumerate(splits):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"Length of training set: {len(train_index)} samples\")\n",
    "    print(f\"Length of testing set (fold): {len(test_index)} samples\")\n",
    "    print(f\"Unique subjects in test set: {np.unique(subjects[test_index])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the classification. We try KNN, Random Forests, XGBoost and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1...\n",
      "Fold 2...\n",
      "Fold 3...\n",
      "Fold 4...\n",
      "Fold 5...\n",
      "Done!\n",
      "For k = 1, KNN mean accuracy = 75.55%\n",
      "For k = 2, KNN mean accuracy = 74.63%\n",
      "For k = 3, KNN mean accuracy = 76.00%\n",
      "For k = 4, KNN mean accuracy = 77.37%\n",
      "For k = 5, KNN mean accuracy = 77.54%\n",
      "For k = 6, KNN mean accuracy = 77.66%\n",
      "For k = 7, KNN mean accuracy = 77.60%\n",
      "For k = 8, KNN mean accuracy = 77.31%\n",
      "For k = 9, KNN mean accuracy = 77.15%\n",
      "For k = 10, KNN mean accuracy = 77.26%\n"
     ]
    }
   ],
   "source": [
    "# KNN classifier\n",
    "k_values = np.arange(1,11)\n",
    "accuracies = np.zeros((5, len(k_values)))\n",
    "for i, (train_index, test_index) in enumerate(splits):\n",
    "    x_train, x_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(f\"Fold {i+1}...\")\n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(x_train, y_train)\n",
    "        y_pred = knn.predict(x_test)\n",
    "        acc = accuracy_score(y_test,y_pred)\n",
    "        # print('k:',k,'accuracy:',acc)\n",
    "        accuracies[i,k-1] = acc\n",
    "print(\"Done!\")\n",
    "\n",
    "# Print the mean accuracy for each k\n",
    "for k, accuracy in enumerate(np.mean(accuracies, axis=0)):\n",
    "    print(f\"For k = {k+1}, KNN mean accuracy = {100*accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1...\n",
      "Fold 2...\n",
      "Fold 3...\n",
      "Fold 4...\n",
      "Fold 5...\n",
      "Done!\n",
      "For 10 trees, RF mean accuracy = 81.25%\n",
      "For 100 trees, RF mean accuracy = 83.82%\n",
      "For 250 trees, RF mean accuracy = 84.21%\n",
      "For 500 trees, RF mean accuracy = 84.04%\n",
      "For 1000 trees, RF mean accuracy = 83.99%\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "tree_values = [10, 100, 250, 500, 1000]\n",
    "accuracies = np.zeros((5, len(tree_values)))\n",
    "for i, (train_index, test_index) in enumerate(splits):\n",
    "    x_train, x_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(f\"Fold {i+1}...\")\n",
    "    for j, trees in enumerate(tree_values):\n",
    "        rf = RandomForestClassifier(n_estimators=trees)\n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracies[i,j] = acc\n",
    "print(\"Done!\")\n",
    "\n",
    "# Print the mean accuracy for each tree number\n",
    "for j, accuracy in enumerate(np.mean(accuracies, axis=0)):\n",
    "    print(f\"For {tree_values[j]} trees, RF mean accuracy = {100*accuracy:.2f}%\")\n",
    "# print(classification_report(y_test, y_pred, target_names=['Walking', 'Sitting Down', 'Standing Up', 'Picking up an Object', 'Drinking Water', 'Falling']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1...\n",
      "Fold 2...\n",
      "Fold 3...\n",
      "Fold 4...\n",
      "Fold 5...\n",
      "Done!\n",
      "For 10 trees, XGBoost mean accuracy = 84.45%\n",
      "For 100 trees, XGBoost mean accuracy = 83.53%\n",
      "For 250 trees, XGBoost mean accuracy = 83.59%\n",
      "For 500 trees, XGBoost mean accuracy = 83.47%\n",
      "For 1000 trees, XGBoost mean accuracy = 83.47%\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "tree_values = [10, 100, 250, 500, 1000]\n",
    "accuracies = np.zeros((5, len(tree_values)))\n",
    "for i, (train_index, test_index) in enumerate(splits):\n",
    "    x_train, x_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(f\"Fold {i+1}...\")\n",
    "    for j, trees in enumerate(tree_values):\n",
    "        xgb = XGBClassifier(n_estimators=trees)\n",
    "        xgb.fit(x_train, y_train)\n",
    "        y_pred = xgb.predict(x_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracies[i,j] = acc\n",
    "print(\"Done!\")\n",
    "\n",
    "# Print the mean accuracy for each tree number\n",
    "for j, accuracy in enumerate(np.mean(accuracies, axis=0)):\n",
    "    print(f\"For {tree_values[j]} trees, XGBoost mean accuracy = {100*accuracy:.2f}%\")\n",
    "# print(classification_report(y_test, y_pred, target_names=['Walking', 'Sitting Down', 'Standing Up', 'Picking up an Object', 'Drinking Water', 'Falling']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1...\n",
      "Fold 2...\n",
      "Fold 3...\n",
      "Fold 4...\n",
      "Fold 5...\n",
      "Done!\n",
      "For linear kernel, SVM mean accuracy = 80.73%\n",
      "For poly kernel, SVM mean accuracy = 69.39%\n",
      "For rbf kernel, SVM mean accuracy = 83.14%\n",
      "For sigmoid kernel, SVM mean accuracy = 60.11%\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "accuracies = np.zeros((5, len(kernels)))\n",
    "for i, (train_index, test_index) in enumerate(splits):\n",
    "    x_train, x_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(f\"Fold {i+1}...\")\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    for j, kernel in enumerate(kernels):\n",
    "        svc = SVC(kernel=kernels[j], random_state=42)\n",
    "        svc.fit(x_train_scaled, y_train)\n",
    "        y_pred = svc.predict(x_test_scaled)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracies[i,j] = acc\n",
    "print(\"Done!\")\n",
    "\n",
    "# Print the mean accuracy for each kernel\n",
    "for j, accuracy in enumerate(np.mean(accuracies, axis=0)):\n",
    "    print(f\"For {kernels[j]} kernel, SVM mean accuracy = {100*accuracy:.2f}%\")\n",
    "# print(classification_report(y_test, y_pred, target_names=['Walking', 'Sitting Down', 'Standing Up', 'Picking up an Object', 'Drinking Water', 'Falling']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
